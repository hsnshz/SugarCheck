{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms Comparitive Analysis for prediction of Diabetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# from scikeras.wrappers import KerasClassifier\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../Dataset/diabetes_data_upload.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for data types\n",
    "df.info()\n",
    "\n",
    "# Coverting columns to lower case\n",
    "df.columns = map(str.lower, df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes distribution by age\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df[\"age\"], bins=30, color=\"royalblue\", kde=True)\n",
    "plt.title(\"Diabetes distribution by age\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes distribution by gender\n",
    "plt.title(\"Diabetes cases by Gender\", fontsize=17)\n",
    "sns.countplot(x=\"gender\", hue=\"class\", data=df, palette=[\"#ff8080\", \"#78ebdc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plots for symptoms and diabetes cases\n",
    "count = 1\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.suptitle(\"Correlation between Symptoms and Diabetes Cases\" + \"\\n\", fontsize=20)\n",
    "for i in df.columns:\n",
    "    if i not in [\"class\", \"age\", \"gender\"]:\n",
    "        plt.subplot(5, 4, count)\n",
    "        plt.title(f\"{i.title()}\", fontweight=\"bold\", fontsize=14)\n",
    "        count += 1\n",
    "        plt.tight_layout()\n",
    "        total_count = df[i].value_counts()\n",
    "        total_percentage = total_count / total_count.sum() * 100\n",
    "        total_percentage.plot(kind=\"bar\", color=[\"royalblue\", \"lightblue\"])\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(f\"{i.title()}\", fontweight=\"bold\", fontsize=12)\n",
    "        for j, v in enumerate(total_percentage):\n",
    "            plt.text(j, v, f\"{v:.1f}%\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing positive to 1 and negative to 0\n",
    "df[\"class\"] = df[\"class\"].replace({\"Positive\": 1, \"Negative\": 0})\n",
    "\n",
    "# Seperating target feature from dataset\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]\n",
    "\n",
    "# Storing features to list\n",
    "objectList = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "print(objectList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for object to numeric transformation\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == \"object\":\n",
    "        X[col] = encoder.fit_transform(X[col])\n",
    "\n",
    "# Printing datatypes of features\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing first 5 rows of dataset\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between features and diabetes\n",
    "X.corrwith(y).plot(\n",
    "    kind=\"bar\", grid=True, figsize=(8, 4), title=\"Correlation with Diabetes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=40\n",
    ")\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "X_train[[\"age\"]] = minmax.fit_transform(X_train[[\"age\"]])\n",
    "X_test[[\"age\"]] = minmax.transform(X_test[[\"age\"]])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(random_state=0, penalty=\"l2\")\n",
    "lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "score = \"accuracy\"\n",
    "\n",
    "acc_lg = model_selection.cross_val_score(lg, X_train, y_train, cv=kfold, scoring=score)\n",
    "acc_lg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "y_pred_lg = lg.predict(X_test)\n",
    "\n",
    "lg_acc = accuracy_score(y_test, y_pred_lg)\n",
    "lg_roc = roc_auc_score(y_test, y_pred_lg)\n",
    "lg_prec = precision_score(y_test, y_pred_lg)\n",
    "lg_rec = recall_score(y_test, y_pred_lg)\n",
    "lg_f1 = f1_score(y_test, y_pred_lg)\n",
    "\n",
    "results_lg = pd.DataFrame(\n",
    "    [[\"Logistic Regression\", lg_acc, acc_lg.mean(), lg_roc, lg_prec, lg_rec, lg_f1]],\n",
    "    columns=[\n",
    "        \"Model\",\n",
    "        \"Accuracy\",\n",
    "        \"Cross Val Accuracy\",\n",
    "        \"ROC\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"F1 Score\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "results_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Trees Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "acc_dt = model_selection.cross_val_score(dt, X_train, y_train, cv=kfold, scoring=score)\n",
    "acc_dt.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "dt_acc = accuracy_score(y_test, y_pred_dt)\n",
    "dt_roc = roc_auc_score(y_test, y_pred_dt)\n",
    "dt_prec = precision_score(y_test, y_pred_dt)\n",
    "dt_rec = recall_score(y_test, y_pred_dt)\n",
    "dt_f1 = f1_score(y_test, y_pred_dt)\n",
    "\n",
    "results_dt = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            \"Decision Trees Classifier\",\n",
    "            dt_acc,\n",
    "            acc_dt.mean(),\n",
    "            dt_roc,\n",
    "            dt_prec,\n",
    "            dt_rec,\n",
    "            dt_f1,\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\n",
    "        \"Model\",\n",
    "        \"Accuracy\",\n",
    "        \"Cross Val Accuracy\",\n",
    "        \"ROC\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"F1 Score\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "results_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = 100\n",
    "\n",
    "for i in range(1, estimators):\n",
    "    rf = RandomForestClassifier(n_estimators=i, criterion=\"entropy\", random_state=0)\n",
    "    rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "acc_rf = model_selection.cross_val_score(rf, X_train, y_train, cv=kfold, scoring=score)\n",
    "acc_rf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "rf_roc = roc_auc_score(y_test, y_pred_rf)\n",
    "rf_prec = precision_score(y_test, y_pred_rf)\n",
    "rf_rec = recall_score(y_test, y_pred_rf)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "results_rf = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            \"Random Forest Classifier\",\n",
    "            rf_acc,\n",
    "            acc_rf.mean(),\n",
    "            rf_roc,\n",
    "            rf_prec,\n",
    "            rf_rec,\n",
    "            rf_f1,\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\n",
    "        \"Model\",\n",
    "        \"Accuracy\",\n",
    "        \"Cross Val Accuracy\",\n",
    "        \"ROC\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"F1 Score\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "results_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(16, input_dim=16, activation=\"relu\"),\n",
    "            Dropout(0.5),\n",
    "            Dense(8, activation=\"relu\"),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "nn = create_model()\n",
    "nn.fit(X_train, y_train, epochs=100, batch_size=10, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to create model, required for KerasClassifier\n",
    "# def create_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(12, input_dim=16, activation='relu'))\n",
    "#     model.add(Dense(8, activation='relu'))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Wrap Keras model with KerasClassifier\n",
    "# nn = KerasClassifier(build_fn=create_model, epochs=100, batch_size=5, verbose=0)\n",
    "\n",
    "# nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom cross-validation\n",
    "def custom_cross_val_score(model_func, X, y, n_splits=10):\n",
    "    kfold = model_selection.StratifiedKFold(\n",
    "        n_splits=n_splits, shuffle=True, random_state=7\n",
    "    )\n",
    "    scores = []\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(X, y):\n",
    "        model = clone_model(model_func())\n",
    "        model.compile(\n",
    "            optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "        X_train_fold, X_test_fold = X[train_ix], X[test_ix]\n",
    "        y_train_fold, y_test_fold = y[train_ix], y[test_ix]\n",
    "        model.fit(X_train_fold, y_train_fold, epochs=100, batch_size=10, verbose=1)\n",
    "        scores.append(model.evaluate(X_test_fold, y_test_fold, verbose=0)[1])\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "acc_nn = custom_cross_val_score(create_model, X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "y_pred_nn = nn.predict(X_test).ravel()\n",
    "y_pred_nn_classes = (y_pred_nn > 0.5).astype(int)\n",
    "\n",
    "nn_acc = accuracy_score(y_test, y_pred_nn_classes)\n",
    "nn_roc = roc_auc_score(y_test, y_pred_nn)\n",
    "nn_prec = precision_score(y_test, y_pred_nn_classes)\n",
    "nn_rec = recall_score(y_test, y_pred_nn_classes)\n",
    "nn_f1 = f1_score(y_test, y_pred_nn_classes)\n",
    "\n",
    "results_nn = pd.DataFrame(\n",
    "    [[\"Neural Network\", nn_acc, acc_nn, nn_roc, nn_prec, nn_rec, nn_f1]],\n",
    "    columns=[\n",
    "        \"Model\",\n",
    "        \"Accuracy\",\n",
    "        \"Cross Val Accuracy\",\n",
    "        \"ROC\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"F1 Score\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "results_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=2)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "acc_knn = model_selection.cross_val_score(\n",
    "    knn, X_train, y_train, cv=kfold, scoring=score\n",
    ")\n",
    "acc_knn.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "knn_acc = accuracy_score(y_test, y_pred_knn)\n",
    "knn_roc = roc_auc_score(y_test, y_pred_knn)\n",
    "knn_prec = precision_score(y_test, y_pred_knn)\n",
    "knn_rec = recall_score(y_test, y_pred_knn)\n",
    "knn_f1 = f1_score(y_test, y_pred_knn)\n",
    "\n",
    "results_knn = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            \"K-Nearest Neighbors\",\n",
    "            knn_acc,\n",
    "            acc_knn.mean(),\n",
    "            knn_roc,\n",
    "            knn_prec,\n",
    "            knn_rec,\n",
    "            knn_f1,\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\n",
    "        \"Model\",\n",
    "        \"Accuracy\",\n",
    "        \"Cross Val Accuracy\",\n",
    "        \"ROC\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"F1 Score\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "results_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store the results\n",
    "final_results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Model\",\n",
    "        \"Accuracy\",\n",
    "        \"Cross Val Accuracy\",\n",
    "        \"ROC\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"F1 Score\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# List of models and their names\n",
    "models = [lg, dt, rf, nn, knn]\n",
    "model_names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"Decision Trees Classifier\",\n",
    "    \"Random Forest Classifier\",\n",
    "    \"Neural Networks\",\n",
    "    \"KNN\",\n",
    "]\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    # Predict the test set results\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if name == \"Neural Networks\":\n",
    "        y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # Calculate the evaluation metrics\n",
    "    acc = accuracy_score(y_test, y_pred) * 100\n",
    "    roc = roc_auc_score(y_test, y_pred) * 100\n",
    "    prec = precision_score(y_test, y_pred) * 100\n",
    "    rec = recall_score(y_test, y_pred) * 100\n",
    "    f1 = f1_score(y_test, y_pred) * 100\n",
    "\n",
    "    # Append the results to the DataFrame\n",
    "    final_results = pd.concat(\n",
    "        [\n",
    "            final_results,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Model\": [name],\n",
    "                    \"Accuracy\": [acc],\n",
    "                    \"Cross Val Accuracy\": [acc.mean()],\n",
    "                    \"ROC\": [roc],\n",
    "                    \"Precision\": [prec],\n",
    "                    \"Recall\": [rec],\n",
    "                    \"F1 Score\": [f1],\n",
    "                }\n",
    "            ),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "final_results.plot(\n",
    "    x=\"Model\",\n",
    "    y=\"Accuracy\",\n",
    "    kind=\"bar\",\n",
    "    title=\"Comparison of Model Accuracies\",\n",
    "    xlabel=\"Models\",\n",
    "    rot=20,\n",
    ")\n",
    "plt.ylabel(\"Accuracy\", rotation=0, ha=\"right\")\n",
    "plt.gca().get_legend().remove()\n",
    "\n",
    "# Add percentage labels to the bars\n",
    "for i, v in enumerate(final_results[\"Accuracy\"]):\n",
    "    plt.text(i, v, f\"{v:.2f}\", ha=\"right\", va=\"bottom\")\n",
    "\n",
    "# Print the final results\n",
    "final_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
